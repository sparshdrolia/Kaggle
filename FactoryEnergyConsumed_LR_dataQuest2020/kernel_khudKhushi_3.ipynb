{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing, model_selection, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata = pd.read_csv(\"/kaggle/input/dataquest2020/energy_train.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of rows in dataset is - ' , data.shape[0])\nprint('The number of columns in dataset is - ' , data.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum().sort_values(ascending = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['degree_C1'].fillna((data['degree_C1'].mean()), inplace=True)\ndata['degree_C3'].fillna((data['degree_C3'].mean()), inplace=True)\ndata['moisture_9'].fillna((data['moisture_9'].mean()), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 75% of the data is usedfor the training of the models and the rest is used for testing\ntrain, test = train_test_split(data,test_size=0.25,random_state=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_time=[\"date\"]\n\ncol_temp = [\"degree_C1\",\"degree_C2\",\"degree_C3\",\"degree_C4\",\"degree_C5\",\"degree_C6\",\"degree_C7\",\"degree_C8\",\"degree_C9\"]\n\ncol_hum = [\"moisture_1\",\"moisture_2\",\"moisture_3\",\"moisture_4\",\"moisture_5\",\"moisture_6\",\"moisture_7\",\"moisture_8\",\"moisture_9\"]\n\ncol_weather = [\"degree_Cout\", \"dew_index\",\"moisture_out\",\"Pressure\",\n                \"Wind\",\"Clarity\"] \n\ncol_light = [\"luminousity\"]\n\ncol_randoms = [\"random_variable_1\", \"random_variable_2\"]\n\ncol_target = [\"WattHour\"]\n\n# Seperate dependent and independent variables \nfeature_vars = train[col_time + col_temp + col_hum + col_weather + col_light + col_randoms ]\ntarget_vars = train[col_target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Due to lot of zero enteries this column is of not much use and will be ignored in rest of the model\n_ = feature_vars.drop(['luminousity'], axis=1 , inplace= True) ;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_vars.head(2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata['WEEKDAY'] = ((pd.to_datetime(data['date']).dt.dayofweek)// 5 == 1).astype(float)\n# There are 5472 weekend recordings \ndata['WEEKDAY'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of all the features to understand the distribution\nfeature_vars.hist(bins = 20 , figsize= (12,16)) ;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2,2,figsize=(12,8))\nvis1 = sns.distplot(feature_vars[\"degree_C6\"],bins=10, ax= ax[0][0])\nvis2 = sns.distplot(feature_vars[\"moisture_out\"],bins=10, ax=ax[0][1])\nvis3 = sns.distplot(feature_vars[\"Clarity\"],bins=10, ax=ax[1][0])\nvis4 = sns.distplot(feature_vars[\"Wind\"],bins=10, ax=ax[1][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of values in Applainces column\nf = plt.figure(figsize=(12,5))\nplt.xlabel('Appliance consumption in Wh')\nplt.ylabel('Frequency')\nsns.distplot(target_vars , bins=10 ) ;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appliance column range with consumption less than 200 Wh\nprint('Percentage of the appliance consumption is less than 200 Wh')\nprint(((target_vars[target_vars <= 200].count()) / (len(target_vars)))*100 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Use the weather , temperature , applainces and random column to see the correlation\ntrain_corr = train[col_temp + col_hum + col_weather +col_target+col_randoms]\ncorr = train_corr.corr()\n# Mask the repeated values\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n  \nf, ax = plt.subplots(figsize=(16, 14))\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, annot=True, fmt=\".2f\" , mask=mask,)\n    #Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n    #Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n    #show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\n# Function to get top correlations \n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(train_corr, 40))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split training dataset into independent and dependent varibales\ntrain_X = train[feature_vars.columns]\ntrain_y = train[target_vars.columns]\ntrain_X.drop(['date'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split testing dataset into independent and dependent varibales\ntest_X = test[feature_vars.columns]\ntest_y = test[target_vars.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.drop([\"random_variable_1\",\"random_variable_2\",\"degree_C9\",\"degree_C6\",\"Clarity\"],axis=1 , inplace=True)\ntest_X.drop([\"random_variable_1\",\"random_variable_2\",\"degree_C9\",\"degree_C6\",\"Clarity\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\n\n# Create test and training set by including Appliances column\n\ntrain = train[list(train_X.columns.values) + col_target ]\n\ntest = test[list(test_X.columns.values) + col_target ]\n\n# Create dummy test and training set to hold scaled values\n\nsc_train = pd.DataFrame(columns=train.columns , index=train.index)\n\nsc_train[sc_train.columns] = sc.fit_transform(train)\n\nsc_test= pd.DataFrame(columns=test.columns , index=test.index)\n\nsc_test[sc_test.columns] = sc.fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_test.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove Appliances column from traininig set\n\ntrain_X =  sc_train.drop(['Appliances'] , axis=1)\ntrain_y = sc_train['Appliances']\n\ntest_X =  sc_test.drop(['Appliances'] , axis=1)\ntest_y = sc_test['Appliances']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_indices = np.argsort(grid_search.best_estimator_.feature_importances_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = grid_search.best_estimator_.feature_importances_\nindices = np.argsort(importances)[::-1]\nnames = [train_X.columns[i] for i in indices]\n# Create plot\nplt.figure(figsize=(10,6))\n\n# Create plot title\nplt.title(\"Feature Importance\")\n\n# Add bars\nplt.bar(range(train_X.shape[1]), importances[indices])\n\n# Add feature names as x-axis labels\nplt.xticks(range(train_X.shape[1]), names, rotation=90)\n\n# Show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###LSTM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt\nfrom sklearn.model_selection import train_test_split\nfrom numpy import concatenate\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom pandas import to_datetime\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom matplotlib import pyplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert series to supervised learning\ndef series_to_supervised(dataset, n_in=1, n_out=1, dropnan=True):\n    num_vars = 1 if type(dataset) is list else dataset.shape[1]\n    dataframe = DataFrame(dataset)\n    cols, names = list(), list()\n    \n    # input sequence (t-n, ....t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(dataframe.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(num_vars)]\n    # forecast sequence (t, t+1 .... t+n)\n    for i in range(0, n_out):\n        cols.append(dataframe.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(num_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(num_vars)]\n    \n    # put it all together \n    agg = concat(cols, axis=1)\n    agg.columns = names\n    \n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature=[\"moisture_out\",\"moisture_8\",\"moisture_1\",\"degree_C3\",\"moisture_3\",\"degree_C2\",\"Pressure\",\"moisture_2\",\"moisture_7\",\"degree_C8\",\"moisture_6\",\"moisture_4\",\"moisture_5\",\"degree_Cout\",\"moisture_9\",\n             \"degree_C4\",\"degree_C7\",\"dew_index\",\"Wind\",\"degree_C1\",\"degree_C5\"]\n\ndata1 = data[col_target + col_time + feature]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndata1[\"date\"]=pd.to_datetime(data1[\"date\"])\ndata1 = data1.set_index(['date'], drop=True)\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values=data1.values\nvalues.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize features\nscaler = MinMaxScaler(feature_range=(0,1))\nscaled = scaler.fit_transform(values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reframed = series_to_supervised(scaled, 1, 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reframed.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reframed.drop(reframed.columns[[22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43]], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values = reframed.values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = values[:,:21]\nY = values[:,21]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=0.3)\n\n# reshape input to be 3D [samples, timesteps, features]\nX_Train = X_Train.reshape((X_Train.shape[0], 1, X_Train.shape[1]))\nX_Test = X_Test.reshape((X_Test.shape[0], 1, X_Test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_Test)\nX_Test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# network architecture\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape=(X_Train.shape[1], X_Train.shape[2])))\nmodel.add(Dense(1))\nmodel.compile(loss='mse', optimizer='adam')\n\n# fit\nhistory = model.fit(X_Train, Y_Train, epochs=70, batch_size=10, validation_data=(X_Test, Y_Test), verbose=2, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyplot.plot(history.history['loss'], label='Train')\npyplot.plot(history.history['val_loss'], label='Test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sca=DataFrame(scaled)\nsca.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_test_mse = model.evaluate(X_Test, Y_Test, batch_size=1)\nprint('Test MSE: %f'%lstm_test_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\ny_pred_test_lstm = model.predict(X_Test)\ny_train_pred_lstm = model.predict(X_Train)\nprint(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(Y_Train, y_train_pred_lstm)))\nprint(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(Y_Test, y_pred_test_lstm)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_y_pred_test = model.predict(X_Test)\nplt.figure(figsize=(10, 6))\nplt.plot(Y_Test, label='True')\nplt.plot(y_pred_test_lstm, label='LSTM')\nplt.title(\"LSTM's Prediction\")\nplt.xlabel('Observation')\nplt.ylabel('Appliances scaled')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a prediction\nyhat = model.predict(X_Test)\nX_Test = X_Test.reshape((X_Test.shape[0], 21))\n# invert scaling for forecast\ninv_yhat = np.concatenate((yhat, X_Test[:, -21:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n# invert scaling for actual\nY_Test = Y_Test.reshape((len(Y_Test), 1))\ninv_y = np.concatenate((Y_Test, X_Test[:, -21:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = pd.read_csv(\"/kaggle/input/dataquest2020/energy_test.csv\")\n\ndata2=data2.fillna(data.mean())\n\ndata2 = data2[col_time + feature]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=np.ones((4375,))\ndata2['Watthour']=a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2[\"date\"]=pd.to_datetime(data2[\"date\"])\ndata2 = data2.set_index(['date'], drop=True)\ndata2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values2=data2.values\nvalues2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled2 = scaler.fit_transform(values2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reframed2 = series_to_supervised(scaled2, 1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reframed2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reframed2.drop(reframed2.columns[[22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43]], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values2 = reframed2.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Test_2 = values2[:,:21]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Test_2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Test = X_Test_2.reshape((X_Test_2.shape[0], 1, X_Test_2.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat = model.predict(X_Test)\nX_Test = X_Test.reshape((X_Test.shape[0], 21))\n# invert scaling for forecast\ninv_yhat = np.concatenate((yhat, X_Test[:, -21:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(inv_yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=np.ones((1,))\nb=b*90","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=np.concatenate((inv_yhat, b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = y.astype(int)\n\nsample = pd.read_csv(\"/kaggle/input/dataquest2020/sample_submission.csv\")\nsubmission_df = pd.DataFrame(columns=['id', 'WattHour'])\nsubmission_df['id'] = sample['id']\nsubmission_df['WattHour'] = pred\n\nfor i in range (0,4375):\n    if submission_df[\"WattHour\"][i]%10 != 0:\n        if (submission_df[\"WattHour\"][i]%10) > 7:\n            submission_df[\"WattHour\"][i] = submission_df[\"WattHour\"][i] + (10 - submission_df[\"WattHour\"][i]%10)\n        else:\n            submission_df[\"WattHour\"][i] = submission_df[\"WattHour\"][i] - (submission_df[\"WattHour\"][i]%10)\n\nprint(submission_df)\nsubmission_df.to_csv('output.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}